{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65e7402-8a03-4006-8345-d98de57894fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Union\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d3ea14-d73d-4eef-bb73-9611d727e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_path = './data/classification/ukairia777/finance_data.csv' # 분류 .csv\n",
    "s_path = './data/summarization/aihub_news' # 요약 폴더내.json 들\n",
    "template_path = './templates/multi.json' # 템플릿  .json\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"Smoked-Salmon-s/empathetic_dialogues_ko\" # 싱글/멀티 대화형 데이터셋  허깅페이스 :\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"beomi/llama-2-koen-13b\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-ko-13b-ft-multi-task-t3_l2048_v2\" # v1은 그대로, v2는 데이터 비율 조정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0752fa7c-c901-4d78-85ba-194c43841cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  load_dataset(dataset_name) # train_args.data_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2639554-050c-4ab6-9a51-a40ca0003779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1b80dc17c546a99588b40773b42dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7481a9776d422f958e528050a193d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a668891ae024e189dd6cf99568cf5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4494ca8c0994655bc022bfe33db2b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_dataset= task_classification_dataset(c_path)\n",
    "s_dataset = task_summarization_dataset(s_path)\n",
    "chat_dataset = chat_load_and_preprocess_data(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f6d442-38a9-40a6-bca3-3b47f289c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 4063\n",
      "Test set shape: 783\n",
      "프로그램 실행 시간:  198.401535987854 초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 'output' 값에 대한 각 인덱스 구하기\n",
    "positive_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '긍정']\n",
    "neutral_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '중립']\n",
    "negative_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '부정']\n",
    "\n",
    "# 각 'output' 값에 대한 필요한 수의 샘플 선택\n",
    "positive_train_size = 1263\n",
    "neutral_train_size = 2296\n",
    "negative_train_size = 504\n",
    "\n",
    "positive_test_size = len(positive_indices) - positive_train_size\n",
    "neutral_test_size = len(neutral_indices) - neutral_train_size\n",
    "negative_test_size = len(negative_indices) - negative_train_size\n",
    "\n",
    "# 훈련 데이터 인덱스 선택\n",
    "positive_train_indices = positive_indices[:positive_train_size]\n",
    "neutral_train_indices = neutral_indices[:neutral_train_size]\n",
    "negative_train_indices = negative_indices[:negative_train_size]\n",
    "\n",
    "# 테스트 데이터 인덱스 선택\n",
    "positive_test_indices = positive_indices[positive_train_size:]\n",
    "neutral_test_indices = neutral_indices[neutral_train_size:]\n",
    "negative_test_indices = negative_indices[negative_train_size:]\n",
    "\n",
    "# 훈련 데이터셋 및 테스트 데이터셋 생성\n",
    "train_dataset_indices = positive_train_indices + neutral_train_indices + negative_train_indices\n",
    "test_dataset_indices = positive_test_indices + neutral_test_indices + negative_test_indices\n",
    "\n",
    "train_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in train_dataset_indices] for key in c_dataset.features})\n",
    "test_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in test_dataset_indices] for key in c_dataset.features})\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"프로그램 실행 시간: \", elapsed_time, \"초\")\n",
    "\n",
    "c_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184347bd-b9a1-4985-9e8f-e5cf8b7fc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 4492\n",
      "Test set shape: 5344\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 크기\n",
    "total_samples = len(s_dataset)\n",
    "\n",
    "# 훈련 데이터의 크기\n",
    "train_size = 4492\n",
    "\n",
    "# 훈련 데이터 인덱스 범위 선택\n",
    "train_indices = list(range(train_size))\n",
    "\n",
    "# 테스트 데이터 인덱스 범위 선택 (나머지)\n",
    "test_indices = list(range(train_size, total_samples))\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분할\n",
    "train_dataset = s_dataset.select(train_indices)\n",
    "test_dataset = s_dataset.select(test_indices)\n",
    "\n",
    "# 데이터셋 형태 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "s_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc632ab-acb2-424f-9f63-c77b097a0a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 20562\n",
      "Test set shape: 6100\n",
      "프로그램 실행 시간:  0.2825772762298584 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 각 타입에 해당하는 인덱스를 구합니다.\n",
    "single_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'single']\n",
    "multi2_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_2']\n",
    "multi3_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_3']\n",
    "\n",
    "# 각 타입에 대해 조정할 샘플 수를 정의합니다.\n",
    "single_samples = 6094\n",
    "multi2_samples = 3712\n",
    "multi3_samples = 10756\n",
    "\n",
    "# 샘플 인덱스를 선택합니다.\n",
    "selected_indices = (single_indices[:single_samples] + \n",
    "                    multi2_indices[:multi2_samples] + \n",
    "                    multi3_indices[:multi3_samples])\n",
    "\n",
    "# 선택된 인덱스를 제외한 나머지를 테스트용으로 사용합니다.\n",
    "test_indices = set(range(len(chat_dataset))) - set(selected_indices)\n",
    "\n",
    "# train 및 test 데이터셋을 생성합니다.\n",
    "train_dataset = chat_dataset.select(selected_indices)\n",
    "test_dataset = chat_dataset.select(list(test_indices))\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"프로그램 실행 시간: \", elapsed_time, \"초\")\n",
    "\n",
    "chat_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5953940-65a7-442d-b8d1-c2ec8f08e161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d21b31197db42489e4b733d302b24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_dataset = task_combined_dataset(c_dataset, s_dataset)\n",
    "task_dataset = DatasetDict({'train': task_dataset})\n",
    "task_dataset = task_load_and_preprocess_data(task_dataset, tokenizer)\n",
    "# 두 데이터셋 합치기\n",
    "combined_dataset = concatenate_datasets([chat_dataset,task_dataset])\n",
    "dataset = combined_dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능\n",
    "dataset = dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2322b8bf-1e1d-40b1-927f-b303b974da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'source', 'type', 'text'],\n",
      "    num_rows: 29117\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이전 대화와 현재 대화의 명령어를 참고하여 가장 적절한 응답을 생성해주세요. 일상적인 대화의 경우, 응답 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\\n\\n[이전 대화]\\n### 명령어: 최근에 스트레스 때문에 잠을 잘 못 자고 있어요. 무슨 조언이 있을까요?\\n### 응답: 그게 정말 힘들겠네요. 스트레스로 인해 잠을 못자는 건 정말 힘든 일이에요. 잠을 잘 자려면 일상 생활에서 스트레스를 푸는 시간을 갖는 것이 중요해요. 한번 시도해 볼 만한 확실한 방법이 있을까요?\\n### 명령어: 저는 스트레스 받을 때 운동을 하면 마음이 편해지는 것 같아요. 다른 방법도 있을까요?\\n### 응답: 그렇네요, 운동은 정말 효과적인 스트레스 해소 방법 중 하나에요. 그 외에도 명상을 해보거나 취미 생활을 즐기는 것도 좋은 방법이에요. 어떤 취미가 있나요? 그 취미를 통해 스트레스를 해소해 볼 수도 있을 것 같아요.\\n\\n\\n[현재 대화]\\n### 명령어:\\n취미는 없지만, 명상은 해보고 싶은데 어떻게 시작해야 할까요?\\n\\n### 응답:\\n취미가 없으시다면 명상을 시작하는 것도 좋은 선택이에요. 명상은 마음에 평화를 가져다 주고 스트레스를 해소해 줄 수 있어요. 인터넷에 명상 가이드나 앱이 많이 있어요. 그 중에 한 가지를 선택해서 따라해보는 것도 좋을 것 같아요. 시작하시면 어떨까요?<|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "dataset['text'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd59f40-d623-434b-9358-7be4dbab1beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a4e1ed-a917-4bf0-a7b9-b413cf924e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d254dd-c988-4667-a2e4-90313df5033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 40000\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 2500\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048 #None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "#device_map = {\"\": 0}\n",
    "device_map = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57d6151-d422-43e8-8dc2-2cce98b36f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7ab6d0-1e1f-4540-8a9b-8d4e2f61425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d879bae-445e-4f78-90ba-1da6af37adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04225a85077249ec991408f0cec63ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1308a454-9c68-4872-8472-dff43181cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63fc747-f794-44e4-a924-74964c235105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\peft\\utils\\other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58264798a0164e27aa38dd4c5ab6ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942a7393-007f-47cd-9226-a5c289e7ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43677' max='43677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43677/43677 107:25:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.831000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=43677, training_loss=0.8959474886533888, metrics={'train_runtime': 386736.6422, 'train_samples_per_second': 0.226, 'train_steps_per_second': 0.113, 'total_flos': 1.968589823748096e+18, 'train_loss': 0.8959474886533888, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c946a1-86ab-4c81-b48a-a35f3f70c78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\tokenizer_config.json',\n",
       " 'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\special_tokens_map.json',\n",
       " 'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'models/' +  new_model\n",
    "\n",
    "# Save trained model / tokenizer\n",
    "trainer.model.save_pretrained(save_path)\n",
    "trainer.tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7836e1c-66cc-4a26-8093-82680f7a8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c400d0c-ec1b-4ed9-96d8-27fe9ecbd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_output(prompt):\n",
    "    # 인퍼런스 커스텀\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\") # return_token_type_ids=False\n",
    "    # peft 일시, **tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False) 아래 inputs['input_ids'] 교체 , 위는 삭제 \n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 256,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.1,\n",
    "        early_stopping=True,\n",
    "        eos_token_id= 46332,\n",
    "    )\n",
    "    output = output[0].to(\"cpu\")\n",
    "    return tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c14920-29ed-4152-b5c8-a1a17b83c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'models/' +  new_model\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d586560b-5f97-46a1-9f03-3eeb0b744329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f61c0b80724ea5b6a2522af3de502a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= 'auto',\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d1e05c-a599-4b62-8772-fbac21f03881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./templates/template_v002.json', 'r',  encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c664c-f6ed-4654-a6e0-7a213722861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무엇이든 질문해보세요! \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  안녕?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  안녕하세요! 반갑습니다. 😊 오늘 하루는 어떻게 시작하셨나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:   오늘 하루는 기분 좋게 시작했어..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  기분 좋게 시작하셨다니 저도 기분이 좋아지네요! 😊 오늘 하루 어떤 일을 계획하셨나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  일찍 일어나고, 아침을 먹고, 씻고 여유있게 출근했지!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  와, 정말 하루를 잘 시작하셨네요! 😊 아침을 먹고 출근하는 건 하루를 기분 좋게 시작하는 좋은 방법이에요. 어떤 아침식사를 하셨나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  다이어트 중이라서 견과류와 바나나 하나를 먹었어. 아 그리고 보니 다이어트 관련 기사를 발견 했는데 요약해줄 수 있을까?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  아침식사로 견과류와 바나나를 선택하셨군요! 균형 잡힌 식사를 위해 노력하시는 모습이 멋져요. 😊 다이어트 관련 기사 요약은 어떤 부분이 궁금하신 건가요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  이 뉴스인데 요약해줄래?             즐거움을 포기하지 않으면서 건강 관리를 추구하는 '헬시 플레저(Healthy Pleasure)', '어다행다(어차피 다이어트 할 거면 행복하게 다이어트)' 등이 주요 소비 트렌드로 확산됨에 따라 주류업계 역시 이러한 흐름에 발맞춰 제로 슈거, 저칼로리, 저도주 등을 내세워 시장을 공략하고 있다.  19일 업계에 따르면 오비맥주 '카스 라이트'가 맥주시장내 제로 슈거 열풍을 주도하고 있다.  최근 오비맥주는 제로 슈거와 저칼로리 등의 매력을 강조해 ‘카스 라이트’를 리뉴얼 출시했다. 카스 라이트는 기존 소비자층과 ‘헬시 플레저’ 트렌드를 이끌고 있는 Z세대 소비자들의 이목을 사로잡으며 라이트 맥주 시장의 저변 확대에 나설 게획이다.    '카스 라이트'는 국내 1위 맥주 ‘카스 프레시’의 자매 브랜드로 2010년 출시 이후 라이트 맥주 카테고리를 선도하며 전체 맥주시장에서 10위 내 판매량을 유지하며 소비자들로부터 꾸준히 사랑받고 있다. 열량이 카스 프레시보다 33%가 낮은 100ml 기준 25kcal이며, 알코올 도수는 4.0도다.  이번 리뉴얼은 '제로 슈거(Zero Sugar)', '카스 프레시 대비 33% 낮은 칼로리', '도수' 등 소비자가 필요로 하는 주요 정보들을 패키지에 명확하게 보이도록 변화를 준 것이 특징이다. 또한 패키지 하단부에 적용된 깔끔한 화이트 컬러가 카스 라이트 브랜드 컬러인 하늘색의 바탕색과 대비를 이루며 보다 경쾌한 분위기가 강조됐다. 새 디자인은 캔과 별, 페트 전 제품에 교체 적용됐다.    하이트진로의 '참이슬 후레쉬'는 저도주 트렌드가 확산되는 소비자의 트렌드를 반영해 알코올 도수를 기존 16.5도에서 16도로 낮춘 제품을 출시했다.  참이슬 후레쉬는 이번 리뉴얼을 통해 특허 받은 대나무 활성숯을 활용한 정제과정을 4번에서 5번으로 늘리며, 잡미와 불순물을 한번 더 제거해 본연의 깨끗하고 깔끔한 맛을 강조했다. 이와 함께 지속적인 소비자 조사 결과를 바탕으로 다각적인 테스트와 분석을 진행해 16도로 최적의 주질을 완성했다. 패키지 디자인도 일부 변경하며 대나무 활성숯 활용 메시지를 직관적으로 시각화 했다.  자극적이지 않은 가벼운 술을 즐기는 Z세대의 음주 문화는 다양한 주류와 음료를 섞어 취향에 맞는 맛과 도수를 즐기는 '믹솔로지(Mixology)' 열풍에도 영향을 미쳤다. 1세대 수제맥주 회사인 카브루에서는 고도주인 위스키를 가볍고 간편하게 즐길 수 있는 하이볼 RTD(Ready To Drink) 상품을 출시했다.   카브루는 최근 리얼 스카치 위스키를 베이스로 한 킬트(KILT) 하이볼을 출시했다. 킬트 하이볼은 갓파더(Godfather), 히비스커스(Hibiscus) 총 2종의 플레이버로 출시되며 도수는 4.5도다. 킬트 하이볼 갓파더는 ‘갓파더’는 아몬드 풍미와 함께 가니쉬로 활용되는 시나몬의 향기를 더했으며, 킬트 하이볼 히비스커스는 청량한 탄산과 어우러지는 은은한 히비스커스 향미로 달콤새콤한 맛이 특징으로 꼽힌다.   이충우 MTN 머니투데이방송 기자\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  즐거움을 포기하지 않으면서 건강 관리를 추구하는 '헬시 플레저(Healthy Pleasure)', '어다행다(어차피 다이어트 할 거면 행복하게 다이어트)' 등이 주요 소비 트렌드로 확산됨에 따라 주류업계 역시 이러한 흐름에 발맞춰 제로 슈거, 저칼로리, 저도주 등을 내세워 시장을 공략하고 있다. 19일 업계에 따르면 오비맥주 '카스 라이트'가 맥주시장내 제로 슈거 열풍을 주도하고 있다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  그러면, 위의 내용의 감성을 분석하고, 그것이 긍정, 중립, 아니면 부정인지 결정하고 대답해줘. 해당 감정 레이블은 \"긍정\", \"중립\" 또는 \"부정\"이야.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  긍정\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:   흠... 저칼로리도 중요하지. 유산소 운동까지 해서 꼭 정상 체중에 도달해야겠어.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  그렇군요, 건강한 식습관과 유산소 운동까지 병행하신다니 정말 멋지세요! 👏👏 건강한 식습관과 유산소 운동이 다이어트에 어떤 영향을 미치는지 알고 계신가요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  그래 시간이 없으니, 대화는 이쯤 하자. 나가볼테니 더 안물어봐도 돼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  네, 알겠습니다. 건강하고 행복한 하루 보내세요! 😊\n",
      "\n"
     ]
    }
   ],
   "source": [
    "his_log = ''\n",
    "cnt = 0 \n",
    "print('무엇이든 질문해보세요! ')\n",
    "while True:\n",
    "    query = input('[### YOU]: ')\n",
    "    prompt = data['prompt'].format(history=his_log, instruction=query)\n",
    "    result = inference_output(prompt)\n",
    "    response = result.split('\\n\\n'+data['response_split'])[1]\n",
    "    response = response.replace('<|endoftext|>', '').strip()\n",
    "    print()\n",
    "    print('[### Chatbot]: ',response)\n",
    "    print()\n",
    "    if cnt == 0:\n",
    "        his_log = query  +'\\n'+ response\n",
    "    else:\n",
    "        his_log = his_log +'\\n'+query + '\\n' + response\n",
    "    cnt += 1 \n",
    "    #end = int(input('끝낼꺼면 0 입력'))\n",
    "    #if end == 0:\n",
    "    if cnt == 10:\n",
    "        print('대화종료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529be3c-c6ec-4cc7-b1dc-7949b3f188ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무엇이든 질문해보세요! \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  안녕하세요...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  안녕하세요! 반갑습니다. 오늘 하루는 어떠셨나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  잠을 잘못 자서 그런지 졸립고 피곤해서 시작부터 힘드네요.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  잠을 잘못 자서 피곤하다니, 시작부터 힘드시겠어요. 😢 잠을 잘 못 자는 것은 정말 힘든 일이죠. 어떤 이유로 잠을 잘못 잤는지 말씀해주실 수 있을까요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  이따가 밥 먹을때 맛있는 음식이라도 먹고 기운내야죠 뭐.  요즘 물가가 워낙 오르다보니 선택지가 마땅치 않네요. 관련 기사도 있는데 요약해줄수 있어요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  물가 상승으로 인해 식비 부담이 커진 상황, 정말 힘드시겠어요. 😔 어떤 음식을 선택해야 할지 고민이 되시나요? 혹시 집에서 직접 만들어 먹는 것에 대해 생각해보셨나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  밑의 내용을 요약해주세요.  [서울=뉴시스]남주현 기자 = 감귤과 사과 등 농산물 물가가 크게 뛰고, 산업용 도시가스 요금 오름세에 생산자물가가 두 달 연속 상승했다.  생산자물가는 통상 1~3개월 시차를 두고 소비자물가에 영향을 미치는 만큼 물가 안정에 부정적으로 영향을 미칠 것으로 보인다.  21일 한국은행이 발표한 1월 생산자물가지수는 121.80(2015년 100기준)으로 전월 대비 0.5% 올랐다. 제1차금속제품, 음식료품 등이 내렸지만 농산물과 서비스 등이 오른 이유가 크다.  생산자물가의 전월 대비 상승률은 지난해 7월(0.3%)부터 8월(0.9%)과 9월(0.5%) 석달 연속 반등했지만, 유가 내림세에 10월(-0.1%)과 11월(-0.4%) 하락한 후 12월(0.1%) 다시 반등한 바 있다.  생산자물가는 생산자가 시장에 공급하는 상품과 서비스 등의 가격 변동을 나타내는 것으로 소비자물가지수의 선행지표로 활용된다.  전년 동월대비로는 1.3% 올라 6개월 연속 올랐다. 생산자물가의 전년대비 상승률은 지난해 6월 -0.3%로 31개월 만에 마이너스를 기록한 후 7월(-0.3%)에도 하락했지만, 8월(1.0%)부터 상승 전환했다.  부문별로는 농림수산품 지수가 전월대비 3.8% 오른 151.26을 기록해 역대 최고치를 기록했다. 축산물(-1.3%)이 내렸지만, 농산물(8.3%)과 수산물(0.2%)이 오르면서다.  세부적으로 감귤은 전월대비 48.8%, 사과는 7.5% 올랐고, 냉동오징어(2.8%)과 김(6.8%)도 상승했다.  공산품은 1차금속제품(-1.0%), 음식료품(-0.3%)이 내렸지만, 석탄및석유제품(0.5%)과 컴퓨터·전자및광학기기(0.9%) 등이 올라 전월대비 0.1% 상승했다.  전력·가스·수도및폐기물은 산업용도시가스(10.0%) 등이 올라 전월대비 1.0% 상승했고, 서비스는 정보통신및방송서비스(1.6%), 사업지원서비스(1.1%), 부동산서비스(0.2%) 등이 상승하며 전월대비 0.6% 올랐다.  유성욱 한은 경제통계국 물가통계팀장은 \"과실류 상승의 주된 이유는 작황 부진으로 제철 과일에 대한 수요도 영향을 미쳤다\"면서 \"감귤은 대체 수요가 되는 사과와 배 물가가 오르면서 같이 올랐다\"고 말했다.  생산자물가와 수입물가지수를 결합해 산출한 1월 국내 공급물가지수는 전월 대비 0.5% 상승했다. 전년 동월과 비교해서는 1.4% 떨어졌다. 원재료(-1.5%)가 내렸고, 중간재(0.6%)와 최종재(0.8%)는 상승했다.  국내 출하를 제외한 수출을 포함하는 총산출 기준으로 상품과 서비스의 가격 변동을 측정한 총산출 물가지수는 전월 대비 0.1% 상승했다. 공산품(1.1%), 서비스(0.6%), 농림수산품(3.8%) 등이 올랐다.   ◎공감언론 뉴시스 njh32@newsis.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  생산자물가는 생산자가 시장에 공급하는 상품과 서비스 등의 가격 변동을 나타내는 것으로 소비자물가지수의 선행지표로 활용된다. 부문별로는 농림수산품 지수가 전월대비 3.8% 오른 151.26을 기록해 역대 최고치를 기록했다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  위의 내용의 감성을 분석하고, 그것이 긍정, 중립, 아니면 부정인지 결정하고 대답해주세요. 해당 감정 레이블은 \"긍정\", \"중립\" 또는 \"부정\"입니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  긍정\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  좋아하는 감귤도 먹기 힘들겠네요... 그래도 열심히 살아야죠.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  맞아요, 물가가 상승하면 좋아하는 과일조차도 부담스러워지는군요. 😢 그럼에도 불구하고 열심히 살아야 한다는 마음가짐, 정말 멋지십니다. 감귤 외에도 좋아하는 과일이 있나요?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  이만, 대화는 종료해주세요. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  네, 알겠습니다. 😊 좋은 하루 보내세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "his_log = ''\n",
    "cnt = 0 \n",
    "print('무엇이든 질문해보세요! ')\n",
    "while True:\n",
    "    query = input('[### YOU]: ')\n",
    "    prompt = data['prompt'].format(history=his_log, instruction=query)\n",
    "    result = inference_output(prompt)\n",
    "    response = result.split('\\n\\n'+data['response_split'])[1]\n",
    "    response = response.replace('<|endoftext|>', '').strip()\n",
    "    print()\n",
    "    print('[### Chatbot]: ',response)\n",
    "    print()\n",
    "    if cnt == 0:\n",
    "        his_log = query  +'\\n'+ response\n",
    "    else:\n",
    "        his_log = his_log +'\\n'+query + '\\n' + response\n",
    "    cnt += 1 \n",
    "    #end = int(input('끝낼꺼면 0 입력'))\n",
    "    #if end == 0:\n",
    "    if cnt == 10:\n",
    "        print('대화종료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc1b70-6c5e-44c9-8cd7-9f7591e625bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7331da-6df4-4d60-8121-a0ff3229e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d9b3a-2af7-4658-b8ba-391c9107d8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd6e5f6-e16f-4d5f-826e-5dafb678931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_classification_dataset(c_path):\n",
    "  # Read the CSV file into a pandas DataFrame\n",
    "  df = pd.read_csv(c_path)\n",
    "  # Replace values in the 'labels' column\n",
    "  df['labels'] = df['labels'].replace({'neutral': '중립', 'negative': '부정', 'positive': '긍정'})\n",
    "\n",
    "  ## 허깅페이스 데이터셋화\n",
    "\n",
    "  # Hugging Face의 dataset 포맷으로 변환\n",
    "  huggingface_dataset = Dataset.from_pandas(df.rename(columns={'labels': 'output', 'kor_sentence': 'input'}))\n",
    "\n",
    "  # instruction 추가 및 컬럼 순서 변경\n",
    "  huggingface_dataset = huggingface_dataset.map(\n",
    "      lambda example: {\"instruction\": '밑의 내용의 감성을 분석하고, 그것이 긍정, 중립, 아니면 부정인지 결정하고 대답해주세요. 해당 감정 레이블은 \"긍정\", \"중립\" 또는 \"부정\"입니다.',\n",
    "                       \"input\": example[\"input\"],\n",
    "                       \"output\": example[\"output\"]\n",
    "                       },\n",
    "      remove_columns=['sentence']\n",
    "  )\n",
    "\n",
    "  # 필드와 값을 추가\n",
    "  sentiment_dataset = huggingface_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'github/ukairia777', 'type': 'task_classification'})\n",
    "\n",
    "  return sentiment_dataset\n",
    "\n",
    "def task_summarization_dataset(s_path):\n",
    "  folder_path  = s_path\n",
    "\n",
    "  # Initialize an empty list to store data\n",
    "  data_list = []\n",
    "\n",
    "  # Get a list of all files in the folder\n",
    "  file_list = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "  # Loop through each JSON file and extract 'passage' and 'summary2'\n",
    "  for file_name in file_list:\n",
    "      file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "      # Read the JSON file\n",
    "      with open(file_path, 'r', encoding='utf-8') as file:\n",
    "          data = json.load(file)\n",
    "\n",
    "      # Extract 'passage' and 'summary2' fields\n",
    "      passage = data.get('Meta(Refine)', {}).get('passage', '')\n",
    "      summary2 = data.get('Annotation', {}).get('summary2', '')\n",
    "\n",
    "      #if len(summary2.split('. ')) <= 1: # 원인 불명 ?  그냥 문장 길이 미만인 거만 짜르도록\n",
    "      if len(summary2) <= 100: # 문장 길이가 100 이하인 것은 버리기\n",
    "        continue\n",
    "\n",
    "      # Append data to the list\n",
    "      data_list.append({'file_name': file_name, 'body': passage, 'summary': summary2})\n",
    "\n",
    "  # Convert the list of dictionaries to a pandas DataFrame\n",
    "  df = pd.DataFrame(data_list)\n",
    "\n",
    "  # Hugging Face의 dataset으로 변환\n",
    "  hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "  # 필요한 형태로 변환\n",
    "  hf_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': '밑의 내용을 요약해주세요.',\n",
    "                       'input': example['body'],\n",
    "                       'output': example['summary']\n",
    "                       },\n",
    "      remove_columns=['file_name','body','summary']\n",
    "                     )\n",
    "  # 필드와 값을 추가\n",
    "  summary_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'aihub', 'type': 'task_summarization'})\n",
    "  return summary_dataset\n",
    "\n",
    "\n",
    "def task_combined_dataset(c_dataset, s_dataset):\n",
    "  # 두 데이터셋 합치기\n",
    "  combined_dataset = concatenate_datasets([c_dataset, s_dataset])\n",
    "  # 데이터셋 섞기\n",
    "  combined_dataset = combined_dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능\n",
    "  # instruction과 input을 합쳐서 새로운 instruction에 값을 주기\n",
    "  combined_dataset = combined_dataset.map(lambda example: {'new_instruction': example['instruction'].replace('#','').strip() + '\\n\\n' + example['input'].rstrip(), 'output': example['output'], 'source': example['source'], 'type': example['type']})\n",
    "\n",
    "  # 기존의 instruction과 input 삭제\n",
    "  combined_dataset = combined_dataset.remove_columns(['instruction', 'input'])\n",
    "  # 'new_instruction' 필드의 이름을 'instruction'으로 변경\n",
    "  combined_dataset = combined_dataset.rename_column('new_instruction', 'instruction')\n",
    "\n",
    "  return combined_dataset\n",
    "\n",
    "\n",
    "class Chat_prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            template_name =  'template_v002' # \"multi\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name, encoding='utf-8') as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "\n",
    "        # 멀티 턴 대화 처리하는 부분\n",
    "        def converter(sentence):\n",
    "            result = re.sub(r\"질문\\s*\", \"### 명령어\", sentence)\n",
    "            result = re.sub(r\"답변\\s*\", \"### 응답\", result)\n",
    "\n",
    "            return result\n",
    "\n",
    "        instruction = converter(instruction)\n",
    "        new_instruction = instruction.split('\\n')[-1]\n",
    "        history = instruction[:-len(new_instruction)]\n",
    "        try:\n",
    "            new_instruction = new_instruction.split('### 명령어: ')[1]\n",
    "        except:\n",
    "            new_instruction = new_instruction.split('### 명령어: ')[0]\n",
    "\n",
    "        res = self.template[\"prompt\"].format(history=history, instruction=new_instruction)\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "def chat_load_and_preprocess_data(dataset: dataset, tokenizer: tokenizer):\n",
    "    data = dataset\n",
    "    prompter = Chat_prompter(template_name = 'template_v002',\n",
    "                        verbose = False)\n",
    "    # prompter.template =>\n",
    "    def generate_and_tokenize_prompt(instruction, output):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            instruction = instruction,\n",
    "            label = output,\n",
    "        )\n",
    "        #full_prompt_add_eos_token = '<s>\\n' + full_prompt + '\\n</s>'\n",
    "        #full_prompt_add_eos_token = full_prompt + '</s>'\n",
    "        full_prompt_add_eos_token = full_prompt + '<|endoftext|>'\n",
    "        return full_prompt_add_eos_token\n",
    "\n",
    "    original_dataset = data['train']\n",
    "    # 새로운 데이터셋 생성\n",
    "    new_dataset = {\n",
    "      'instruction': original_dataset['instruction'],\n",
    "      'output': original_dataset['output'],\n",
    "      'source': original_dataset['source'],\n",
    "      'type': original_dataset['type'],\n",
    "      'text': [generate_and_tokenize_prompt(i,o) for i, o in zip(original_dataset['instruction'], original_dataset['output'])]\n",
    "    }\n",
    "\n",
    "    # 최종 데이터셋 생성\n",
    "    final_dataset = Dataset.from_dict(new_dataset)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "class Task_prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            template_name =  'template_v002' # \"multi\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name, encoding='utf-8') as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "\n",
    "        # 멀티 턴 대화 처리하는 부분\n",
    "        def converter(sentence):\n",
    "            result = re.sub(r\"질문\\s*\", \"### 명령어\", sentence)\n",
    "            result = re.sub(r\"답변\\s*\", \"### 응답\", result)\n",
    "\n",
    "            return result\n",
    "\n",
    "        instruction = converter(instruction)\n",
    "        history = ''\n",
    "\n",
    "        res = self.template[\"prompt\"].format(history=history, instruction=instruction)\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "def task_load_and_preprocess_data(dataset: dataset, tokenizer: tokenizer):\n",
    "    data = dataset\n",
    "    prompter = Task_prompter(template_name = 'template_v002',\n",
    "                        verbose = False)\n",
    "    # prompter.template =>\n",
    "    def generate_and_tokenize_prompt(instruction, output):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            instruction = instruction,\n",
    "            label = output,\n",
    "        )\n",
    "\n",
    "        full_prompt_add_eos_token = full_prompt + '<|endoftext|>'\n",
    "        return full_prompt_add_eos_token\n",
    "\n",
    "    original_dataset = data['train']\n",
    "    # 새로운 데이터셋 생성\n",
    "    new_dataset = {\n",
    "      'instruction': original_dataset['instruction'],\n",
    "      'output': original_dataset['output'],\n",
    "      'source': original_dataset['source'],\n",
    "      'type': original_dataset['type'],\n",
    "      'text': [generate_and_tokenize_prompt(i,o) for i, o in zip(original_dataset['instruction'], original_dataset['output'])]\n",
    "    }\n",
    "\n",
    "    # 최종 데이터셋 생성\n",
    "    final_dataset = Dataset.from_dict(new_dataset)\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecf860-1d87-4576-bbe9-84d7d56aeffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oosij",
   "language": "python",
   "name": "oosij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
