{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65e7402-8a03-4006-8345-d98de57894fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Union\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d3ea14-d73d-4eef-bb73-9611d727e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_path = './data/classification/ukairia777/finance_data.csv' # ë¶„ë¥˜ .csv\n",
    "s_path = './data/summarization/aihub_news' # ìš”ì•½ í´ë”ë‚´.json ë“¤\n",
    "template_path = './templates/multi.json' # í…œí”Œë¦¿  .json\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"Smoked-Salmon-s/empathetic_dialogues_ko\" # ì‹±ê¸€/ë©€í‹° ëŒ€í™”í˜• ë°ì´í„°ì…‹  í—ˆê¹…í˜ì´ìŠ¤ :\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"beomi/llama-2-koen-13b\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-ko-13b-ft-multi-task-t3_l2048_v2\" # v1ì€ ê·¸ëŒ€ë¡œ, v2ëŠ” ë°ì´í„° ë¹„ìœ¨ ì¡°ì • "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0752fa7c-c901-4d78-85ba-194c43841cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  load_dataset(dataset_name) # train_args.data_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2639554-050c-4ab6-9a51-a40ca0003779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1b80dc17c546a99588b40773b42dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7481a9776d422f958e528050a193d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a668891ae024e189dd6cf99568cf5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4494ca8c0994655bc022bfe33db2b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_dataset= task_classification_dataset(c_path)\n",
    "s_dataset = task_summarization_dataset(s_path)\n",
    "chat_dataset = chat_load_and_preprocess_data(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f6d442-38a9-40a6-bca3-3b47f289c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 4063\n",
      "Test set shape: 783\n",
      "í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œê°„:  198.401535987854 ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 'output' ê°’ì— ëŒ€í•œ ê° ì¸ë±ìŠ¤ êµ¬í•˜ê¸°\n",
    "positive_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == 'ê¸ì •']\n",
    "neutral_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == 'ì¤‘ë¦½']\n",
    "negative_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == 'ë¶€ì •']\n",
    "\n",
    "# ê° 'output' ê°’ì— ëŒ€í•œ í•„ìš”í•œ ìˆ˜ì˜ ìƒ˜í”Œ ì„ íƒ\n",
    "positive_train_size = 1263\n",
    "neutral_train_size = 2296\n",
    "negative_train_size = 504\n",
    "\n",
    "positive_test_size = len(positive_indices) - positive_train_size\n",
    "neutral_test_size = len(neutral_indices) - neutral_train_size\n",
    "negative_test_size = len(negative_indices) - negative_train_size\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ì„ íƒ\n",
    "positive_train_indices = positive_indices[:positive_train_size]\n",
    "neutral_train_indices = neutral_indices[:neutral_train_size]\n",
    "negative_train_indices = negative_indices[:negative_train_size]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤ ì„ íƒ\n",
    "positive_test_indices = positive_indices[positive_train_size:]\n",
    "neutral_test_indices = neutral_indices[neutral_train_size:]\n",
    "negative_test_indices = negative_indices[negative_train_size:]\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì…‹ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset_indices = positive_train_indices + neutral_train_indices + negative_train_indices\n",
    "test_dataset_indices = positive_test_indices + neutral_test_indices + negative_test_indices\n",
    "\n",
    "train_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in train_dataset_indices] for key in c_dataset.features})\n",
    "test_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in test_dataset_indices] for key in c_dataset.features})\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œê°„: \", elapsed_time, \"ì´ˆ\")\n",
    "\n",
    "c_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184347bd-b9a1-4985-9e8f-e5cf8b7fc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 4492\n",
      "Test set shape: 5344\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì˜ í¬ê¸°\n",
    "total_samples = len(s_dataset)\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì˜ í¬ê¸°\n",
    "train_size = 4492\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ë²”ìœ„ ì„ íƒ\n",
    "train_indices = list(range(train_size))\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤ ë²”ìœ„ ì„ íƒ (ë‚˜ë¨¸ì§€)\n",
    "test_indices = list(range(train_size, total_samples))\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• \n",
    "train_dataset = s_dataset.select(train_indices)\n",
    "test_dataset = s_dataset.select(test_indices)\n",
    "\n",
    "# ë°ì´í„°ì…‹ í˜•íƒœ í™•ì¸\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "s_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc632ab-acb2-424f-9f63-c77b097a0a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 20562\n",
      "Test set shape: 6100\n",
      "í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œê°„:  0.2825772762298584 ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# ê° íƒ€ì…ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
    "single_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'single']\n",
    "multi2_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_2']\n",
    "multi3_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_3']\n",
    "\n",
    "# ê° íƒ€ì…ì— ëŒ€í•´ ì¡°ì •í•  ìƒ˜í”Œ ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "single_samples = 6094\n",
    "multi2_samples = 3712\n",
    "multi3_samples = 10756\n",
    "\n",
    "# ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "selected_indices = (single_indices[:single_samples] + \n",
    "                    multi2_indices[:multi2_samples] + \n",
    "                    multi3_indices[:multi3_samples])\n",
    "\n",
    "# ì„ íƒëœ ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "test_indices = set(range(len(chat_dataset))) - set(selected_indices)\n",
    "\n",
    "# train ë° test ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "train_dataset = chat_dataset.select(selected_indices)\n",
    "test_dataset = chat_dataset.select(list(test_indices))\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œê°„: \", elapsed_time, \"ì´ˆ\")\n",
    "\n",
    "chat_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5953940-65a7-442d-b8d1-c2ec8f08e161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d21b31197db42489e4b733d302b24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_dataset = task_combined_dataset(c_dataset, s_dataset)\n",
    "task_dataset = DatasetDict({'train': task_dataset})\n",
    "task_dataset = task_load_and_preprocess_data(task_dataset, tokenizer)\n",
    "# ë‘ ë°ì´í„°ì…‹ í•©ì¹˜ê¸°\n",
    "combined_dataset = concatenate_datasets([chat_dataset,task_dataset])\n",
    "dataset = combined_dataset.shuffle(seed=42)  # seed ê°’ì€ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "dataset = dataset.shuffle(seed=42)  # seed ê°’ì€ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2322b8bf-1e1d-40b1-927f-b303b974da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'source', 'type', 'text'],\n",
      "    num_rows: 29117\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ ëª…ë ¹ì–´ë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”. ì¼ìƒì ì¸ ëŒ€í™”ì˜ ê²½ìš°, ì‘ë‹µ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\\n\\n[ì´ì „ ëŒ€í™”]\\n### ëª…ë ¹ì–´: ìµœê·¼ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë•Œë¬¸ì— ì ì„ ì˜ ëª» ìê³  ìˆì–´ìš”. ë¬´ìŠ¨ ì¡°ì–¸ì´ ìˆì„ê¹Œìš”?\\n### ì‘ë‹µ: ê·¸ê²Œ ì •ë§ í˜ë“¤ê² ë„¤ìš”. ìŠ¤íŠ¸ë ˆìŠ¤ë¡œ ì¸í•´ ì ì„ ëª»ìëŠ” ê±´ ì •ë§ í˜ë“  ì¼ì´ì—ìš”. ì ì„ ì˜ ìë ¤ë©´ ì¼ìƒ ìƒí™œì—ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í‘¸ëŠ” ì‹œê°„ì„ ê°–ëŠ” ê²ƒì´ ì¤‘ìš”í•´ìš”. í•œë²ˆ ì‹œë„í•´ ë³¼ ë§Œí•œ í™•ì‹¤í•œ ë°©ë²•ì´ ìˆì„ê¹Œìš”?\\n### ëª…ë ¹ì–´: ì €ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì„ ë•Œ ìš´ë™ì„ í•˜ë©´ ë§ˆìŒì´ í¸í•´ì§€ëŠ” ê²ƒ ê°™ì•„ìš”. ë‹¤ë¥¸ ë°©ë²•ë„ ìˆì„ê¹Œìš”?\\n### ì‘ë‹µ: ê·¸ë ‡ë„¤ìš”, ìš´ë™ì€ ì •ë§ íš¨ê³¼ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²• ì¤‘ í•˜ë‚˜ì—ìš”. ê·¸ ì™¸ì—ë„ ëª…ìƒì„ í•´ë³´ê±°ë‚˜ ì·¨ë¯¸ ìƒí™œì„ ì¦ê¸°ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ì—ìš”. ì–´ë–¤ ì·¨ë¯¸ê°€ ìˆë‚˜ìš”? ê·¸ ì·¨ë¯¸ë¥¼ í†µí•´ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•´ ë³¼ ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ì•„ìš”.\\n\\n\\n[í˜„ì¬ ëŒ€í™”]\\n### ëª…ë ¹ì–´:\\nì·¨ë¯¸ëŠ” ì—†ì§€ë§Œ, ëª…ìƒì€ í•´ë³´ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í• ê¹Œìš”?\\n\\n### ì‘ë‹µ:\\nì·¨ë¯¸ê°€ ì—†ìœ¼ì‹œë‹¤ë©´ ëª…ìƒì„ ì‹œì‘í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ì„ íƒì´ì—ìš”. ëª…ìƒì€ ë§ˆìŒì— í‰í™”ë¥¼ ê°€ì ¸ë‹¤ ì£¼ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•´ ì¤„ ìˆ˜ ìˆì–´ìš”. ì¸í„°ë„·ì— ëª…ìƒ ê°€ì´ë“œë‚˜ ì•±ì´ ë§ì´ ìˆì–´ìš”. ê·¸ ì¤‘ì— í•œ ê°€ì§€ë¥¼ ì„ íƒí•´ì„œ ë”°ë¼í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ì‹œì‘í•˜ì‹œë©´ ì–´ë–¨ê¹Œìš”?<|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "dataset['text'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd59f40-d623-434b-9358-7be4dbab1beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a4e1ed-a917-4bf0-a7b9-b413cf924e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d254dd-c988-4667-a2e4-90313df5033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 40000\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 2500\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048 #None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "#device_map = {\"\": 0}\n",
    "device_map = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57d6151-d422-43e8-8dc2-2cce98b36f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7ab6d0-1e1f-4540-8a9b-8d4e2f61425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d879bae-445e-4f78-90ba-1da6af37adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04225a85077249ec991408f0cec63ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1308a454-9c68-4872-8472-dff43181cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63fc747-f794-44e4-a924-74964c235105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\peft\\utils\\other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58264798a0164e27aa38dd4c5ab6ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942a7393-007f-47cd-9226-a5c289e7ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43677' max='43677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43677/43677 107:25:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.831000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=43677, training_loss=0.8959474886533888, metrics={'train_runtime': 386736.6422, 'train_samples_per_second': 0.226, 'train_steps_per_second': 0.113, 'total_flos': 1.968589823748096e+18, 'train_loss': 0.8959474886533888, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c946a1-86ab-4c81-b48a-a35f3f70c78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\tokenizer_config.json',\n",
       " 'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\special_tokens_map.json',\n",
       " 'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'models/' +  new_model\n",
    "\n",
    "# Save trained model / tokenizer\n",
    "trainer.model.save_pretrained(save_path)\n",
    "trainer.tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7836e1c-66cc-4a26-8093-82680f7a8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c400d0c-ec1b-4ed9-96d8-27fe9ecbd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_output(prompt):\n",
    "    # ì¸í¼ëŸ°ìŠ¤ ì»¤ìŠ¤í…€\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\") # return_token_type_ids=False\n",
    "    # peft ì¼ì‹œ, **tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False) ì•„ë˜ inputs['input_ids'] êµì²´ , ìœ„ëŠ” ì‚­ì œ \n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 256,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.1,\n",
    "        early_stopping=True,\n",
    "        eos_token_id= 46332,\n",
    "    )\n",
    "    output = output[0].to(\"cpu\")\n",
    "    return tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c14920-29ed-4152-b5c8-a1a17b83c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/llama-2-ko-13b-ft-multi-task-t3_l2048_v2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'models/' +  new_model\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d586560b-5f97-46a1-9f03-3eeb0b744329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f61c0b80724ea5b6a2522af3de502a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= 'auto',\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d1e05c-a599-4b62-8772-fbac21f03881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./templates/template_v002.json', 'r',  encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c664c-f6ed-4654-a6e0-7a213722861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”! \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì•ˆë…•?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë–»ê²Œ ì‹œì‘í•˜ì…¨ë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:   ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ê¸°ë¶„ ì¢‹ê²Œ ì‹œì‘í–ˆì–´..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ê¸°ë¶„ ì¢‹ê²Œ ì‹œì‘í•˜ì…¨ë‹¤ë‹ˆ ì €ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ë„¤ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë–¤ ì¼ì„ ê³„íší•˜ì…¨ë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì¼ì° ì¼ì–´ë‚˜ê³ , ì•„ì¹¨ì„ ë¨¹ê³ , ì”»ê³  ì—¬ìœ ìˆê²Œ ì¶œê·¼í–ˆì§€!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì™€, ì •ë§ í•˜ë£¨ë¥¼ ì˜ ì‹œì‘í•˜ì…¨ë„¤ìš”! ğŸ˜Š ì•„ì¹¨ì„ ë¨¹ê³  ì¶œê·¼í•˜ëŠ” ê±´ í•˜ë£¨ë¥¼ ê¸°ë¶„ ì¢‹ê²Œ ì‹œì‘í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì´ì—ìš”. ì–´ë–¤ ì•„ì¹¨ì‹ì‚¬ë¥¼ í•˜ì…¨ë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ë‹¤ì´ì–´íŠ¸ ì¤‘ì´ë¼ì„œ ê²¬ê³¼ë¥˜ì™€ ë°”ë‚˜ë‚˜ í•˜ë‚˜ë¥¼ ë¨¹ì—ˆì–´. ì•„ ê·¸ë¦¬ê³  ë³´ë‹ˆ ë‹¤ì´ì–´íŠ¸ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ë°œê²¬ í–ˆëŠ”ë° ìš”ì•½í•´ì¤„ ìˆ˜ ìˆì„ê¹Œ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì•„ì¹¨ì‹ì‚¬ë¡œ ê²¬ê³¼ë¥˜ì™€ ë°”ë‚˜ë‚˜ë¥¼ ì„ íƒí•˜ì…¨êµ°ìš”! ê· í˜• ì¡íŒ ì‹ì‚¬ë¥¼ ìœ„í•´ ë…¸ë ¥í•˜ì‹œëŠ” ëª¨ìŠµì´ ë©‹ì ¸ìš”. ğŸ˜Š ë‹¤ì´ì–´íŠ¸ ê´€ë ¨ ê¸°ì‚¬ ìš”ì•½ì€ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹  ê±´ê°€ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì´ ë‰´ìŠ¤ì¸ë° ìš”ì•½í•´ì¤„ë˜?             ì¦ê±°ì›€ì„ í¬ê¸°í•˜ì§€ ì•Šìœ¼ë©´ì„œ ê±´ê°• ê´€ë¦¬ë¥¼ ì¶”êµ¬í•˜ëŠ” 'í—¬ì‹œ í”Œë ˆì €(Healthy Pleasure)', 'ì–´ë‹¤í–‰ë‹¤(ì–´ì°¨í”¼ ë‹¤ì´ì–´íŠ¸ í•  ê±°ë©´ í–‰ë³µí•˜ê²Œ ë‹¤ì´ì–´íŠ¸)' ë“±ì´ ì£¼ìš” ì†Œë¹„ íŠ¸ë Œë“œë¡œ í™•ì‚°ë¨ì— ë”°ë¼ ì£¼ë¥˜ì—…ê³„ ì—­ì‹œ ì´ëŸ¬í•œ íë¦„ì— ë°œë§ì¶° ì œë¡œ ìŠˆê±°, ì €ì¹¼ë¡œë¦¬, ì €ë„ì£¼ ë“±ì„ ë‚´ì„¸ì›Œ ì‹œì¥ì„ ê³µëµí•˜ê³  ìˆë‹¤.  19ì¼ ì—…ê³„ì— ë”°ë¥´ë©´ ì˜¤ë¹„ë§¥ì£¼ 'ì¹´ìŠ¤ ë¼ì´íŠ¸'ê°€ ë§¥ì£¼ì‹œì¥ë‚´ ì œë¡œ ìŠˆê±° ì—´í’ì„ ì£¼ë„í•˜ê³  ìˆë‹¤.  ìµœê·¼ ì˜¤ë¹„ë§¥ì£¼ëŠ” ì œë¡œ ìŠˆê±°ì™€ ì €ì¹¼ë¡œë¦¬ ë“±ì˜ ë§¤ë ¥ì„ ê°•ì¡°í•´ â€˜ì¹´ìŠ¤ ë¼ì´íŠ¸â€™ë¥¼ ë¦¬ë‰´ì–¼ ì¶œì‹œí–ˆë‹¤. ì¹´ìŠ¤ ë¼ì´íŠ¸ëŠ” ê¸°ì¡´ ì†Œë¹„ìì¸µê³¼ â€˜í—¬ì‹œ í”Œë ˆì €â€™ íŠ¸ë Œë“œë¥¼ ì´ëŒê³  ìˆëŠ” Zì„¸ëŒ€ ì†Œë¹„ìë“¤ì˜ ì´ëª©ì„ ì‚¬ë¡œì¡ìœ¼ë©° ë¼ì´íŠ¸ ë§¥ì£¼ ì‹œì¥ì˜ ì €ë³€ í™•ëŒ€ì— ë‚˜ì„¤ ê²Œíšì´ë‹¤.    'ì¹´ìŠ¤ ë¼ì´íŠ¸'ëŠ” êµ­ë‚´ 1ìœ„ ë§¥ì£¼ â€˜ì¹´ìŠ¤ í”„ë ˆì‹œâ€™ì˜ ìë§¤ ë¸Œëœë“œë¡œ 2010ë…„ ì¶œì‹œ ì´í›„ ë¼ì´íŠ¸ ë§¥ì£¼ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ ë„í•˜ë©° ì „ì²´ ë§¥ì£¼ì‹œì¥ì—ì„œ 10ìœ„ ë‚´ íŒë§¤ëŸ‰ì„ ìœ ì§€í•˜ë©° ì†Œë¹„ìë“¤ë¡œë¶€í„° ê¾¸ì¤€íˆ ì‚¬ë‘ë°›ê³  ìˆë‹¤. ì—´ëŸ‰ì´ ì¹´ìŠ¤ í”„ë ˆì‹œë³´ë‹¤ 33%ê°€ ë‚®ì€ 100ml ê¸°ì¤€ 25kcalì´ë©°, ì•Œì½”ì˜¬ ë„ìˆ˜ëŠ” 4.0ë„ë‹¤.  ì´ë²ˆ ë¦¬ë‰´ì–¼ì€ 'ì œë¡œ ìŠˆê±°(Zero Sugar)', 'ì¹´ìŠ¤ í”„ë ˆì‹œ ëŒ€ë¹„ 33% ë‚®ì€ ì¹¼ë¡œë¦¬', 'ë„ìˆ˜' ë“± ì†Œë¹„ìê°€ í•„ìš”ë¡œ í•˜ëŠ” ì£¼ìš” ì •ë³´ë“¤ì„ íŒ¨í‚¤ì§€ì— ëª…í™•í•˜ê²Œ ë³´ì´ë„ë¡ ë³€í™”ë¥¼ ì¤€ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ë˜í•œ íŒ¨í‚¤ì§€ í•˜ë‹¨ë¶€ì— ì ìš©ëœ ê¹”ë”í•œ í™”ì´íŠ¸ ì»¬ëŸ¬ê°€ ì¹´ìŠ¤ ë¼ì´íŠ¸ ë¸Œëœë“œ ì»¬ëŸ¬ì¸ í•˜ëŠ˜ìƒ‰ì˜ ë°”íƒ•ìƒ‰ê³¼ ëŒ€ë¹„ë¥¼ ì´ë£¨ë©° ë³´ë‹¤ ê²½ì¾Œí•œ ë¶„ìœ„ê¸°ê°€ ê°•ì¡°ëë‹¤. ìƒˆ ë””ìì¸ì€ ìº”ê³¼ ë³„, í˜íŠ¸ ì „ ì œí’ˆì— êµì²´ ì ìš©ëë‹¤.    í•˜ì´íŠ¸ì§„ë¡œì˜ 'ì°¸ì´ìŠ¬ í›„ë ˆì‰¬'ëŠ” ì €ë„ì£¼ íŠ¸ë Œë“œê°€ í™•ì‚°ë˜ëŠ” ì†Œë¹„ìì˜ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•´ ì•Œì½”ì˜¬ ë„ìˆ˜ë¥¼ ê¸°ì¡´ 16.5ë„ì—ì„œ 16ë„ë¡œ ë‚®ì¶˜ ì œí’ˆì„ ì¶œì‹œí–ˆë‹¤.  ì°¸ì´ìŠ¬ í›„ë ˆì‰¬ëŠ” ì´ë²ˆ ë¦¬ë‰´ì–¼ì„ í†µí•´ íŠ¹í—ˆ ë°›ì€ ëŒ€ë‚˜ë¬´ í™œì„±ìˆ¯ì„ í™œìš©í•œ ì •ì œê³¼ì •ì„ 4ë²ˆì—ì„œ 5ë²ˆìœ¼ë¡œ ëŠ˜ë¦¬ë©°, ì¡ë¯¸ì™€ ë¶ˆìˆœë¬¼ì„ í•œë²ˆ ë” ì œê±°í•´ ë³¸ì—°ì˜ ê¹¨ë—í•˜ê³  ê¹”ë”í•œ ë§›ì„ ê°•ì¡°í–ˆë‹¤. ì´ì™€ í•¨ê»˜ ì§€ì†ì ì¸ ì†Œë¹„ì ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ê°ì ì¸ í…ŒìŠ¤íŠ¸ì™€ ë¶„ì„ì„ ì§„í–‰í•´ 16ë„ë¡œ ìµœì ì˜ ì£¼ì§ˆì„ ì™„ì„±í–ˆë‹¤. íŒ¨í‚¤ì§€ ë””ìì¸ë„ ì¼ë¶€ ë³€ê²½í•˜ë©° ëŒ€ë‚˜ë¬´ í™œì„±ìˆ¯ í™œìš© ë©”ì‹œì§€ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™” í–ˆë‹¤.  ìê·¹ì ì´ì§€ ì•Šì€ ê°€ë²¼ìš´ ìˆ ì„ ì¦ê¸°ëŠ” Zì„¸ëŒ€ì˜ ìŒì£¼ ë¬¸í™”ëŠ” ë‹¤ì–‘í•œ ì£¼ë¥˜ì™€ ìŒë£Œë¥¼ ì„ì–´ ì·¨í–¥ì— ë§ëŠ” ë§›ê³¼ ë„ìˆ˜ë¥¼ ì¦ê¸°ëŠ” 'ë¯¹ì†”ë¡œì§€(Mixology)' ì—´í’ì—ë„ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤. 1ì„¸ëŒ€ ìˆ˜ì œë§¥ì£¼ íšŒì‚¬ì¸ ì¹´ë¸Œë£¨ì—ì„œëŠ” ê³ ë„ì£¼ì¸ ìœ„ìŠ¤í‚¤ë¥¼ ê°€ë³ê³  ê°„í¸í•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” í•˜ì´ë³¼ RTD(Ready To Drink) ìƒí’ˆì„ ì¶œì‹œí–ˆë‹¤.   ì¹´ë¸Œë£¨ëŠ” ìµœê·¼ ë¦¬ì–¼ ìŠ¤ì¹´ì¹˜ ìœ„ìŠ¤í‚¤ë¥¼ ë² ì´ìŠ¤ë¡œ í•œ í‚¬íŠ¸(KILT) í•˜ì´ë³¼ì„ ì¶œì‹œí–ˆë‹¤. í‚¬íŠ¸ í•˜ì´ë³¼ì€ ê°“íŒŒë”(Godfather), íˆë¹„ìŠ¤ì»¤ìŠ¤(Hibiscus) ì´ 2ì¢…ì˜ í”Œë ˆì´ë²„ë¡œ ì¶œì‹œë˜ë©° ë„ìˆ˜ëŠ” 4.5ë„ë‹¤. í‚¬íŠ¸ í•˜ì´ë³¼ ê°“íŒŒë”ëŠ” â€˜ê°“íŒŒë”â€™ëŠ” ì•„ëª¬ë“œ í’ë¯¸ì™€ í•¨ê»˜ ê°€ë‹ˆì‰¬ë¡œ í™œìš©ë˜ëŠ” ì‹œë‚˜ëª¬ì˜ í–¥ê¸°ë¥¼ ë”í–ˆìœ¼ë©°, í‚¬íŠ¸ í•˜ì´ë³¼ íˆë¹„ìŠ¤ì»¤ìŠ¤ëŠ” ì²­ëŸ‰í•œ íƒ„ì‚°ê³¼ ì–´ìš°ëŸ¬ì§€ëŠ” ì€ì€í•œ íˆë¹„ìŠ¤ì»¤ìŠ¤ í–¥ë¯¸ë¡œ ë‹¬ì½¤ìƒˆì½¤í•œ ë§›ì´ íŠ¹ì§•ìœ¼ë¡œ ê¼½íŒë‹¤.   ì´ì¶©ìš° MTN ë¨¸ë‹ˆíˆ¬ë°ì´ë°©ì†¡ ê¸°ì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì¦ê±°ì›€ì„ í¬ê¸°í•˜ì§€ ì•Šìœ¼ë©´ì„œ ê±´ê°• ê´€ë¦¬ë¥¼ ì¶”êµ¬í•˜ëŠ” 'í—¬ì‹œ í”Œë ˆì €(Healthy Pleasure)', 'ì–´ë‹¤í–‰ë‹¤(ì–´ì°¨í”¼ ë‹¤ì´ì–´íŠ¸ í•  ê±°ë©´ í–‰ë³µí•˜ê²Œ ë‹¤ì´ì–´íŠ¸)' ë“±ì´ ì£¼ìš” ì†Œë¹„ íŠ¸ë Œë“œë¡œ í™•ì‚°ë¨ì— ë”°ë¼ ì£¼ë¥˜ì—…ê³„ ì—­ì‹œ ì´ëŸ¬í•œ íë¦„ì— ë°œë§ì¶° ì œë¡œ ìŠˆê±°, ì €ì¹¼ë¡œë¦¬, ì €ë„ì£¼ ë“±ì„ ë‚´ì„¸ì›Œ ì‹œì¥ì„ ê³µëµí•˜ê³  ìˆë‹¤. 19ì¼ ì—…ê³„ì— ë”°ë¥´ë©´ ì˜¤ë¹„ë§¥ì£¼ 'ì¹´ìŠ¤ ë¼ì´íŠ¸'ê°€ ë§¥ì£¼ì‹œì¥ë‚´ ì œë¡œ ìŠˆê±° ì—´í’ì„ ì£¼ë„í•˜ê³  ìˆë‹¤.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ê·¸ëŸ¬ë©´, ìœ„ì˜ ë‚´ìš©ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³ , ê·¸ê²ƒì´ ê¸ì •, ì¤‘ë¦½, ì•„ë‹ˆë©´ ë¶€ì •ì¸ì§€ ê²°ì •í•˜ê³  ëŒ€ë‹µí•´ì¤˜. í•´ë‹¹ ê°ì • ë ˆì´ë¸”ì€ \"ê¸ì •\", \"ì¤‘ë¦½\" ë˜ëŠ” \"ë¶€ì •\"ì´ì•¼.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ê¸ì •\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:   í ... ì €ì¹¼ë¡œë¦¬ë„ ì¤‘ìš”í•˜ì§€. ìœ ì‚°ì†Œ ìš´ë™ê¹Œì§€ í•´ì„œ ê¼­ ì •ìƒ ì²´ì¤‘ì— ë„ë‹¬í•´ì•¼ê² ì–´.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ê·¸ë ‡êµ°ìš”, ê±´ê°•í•œ ì‹ìŠµê´€ê³¼ ìœ ì‚°ì†Œ ìš´ë™ê¹Œì§€ ë³‘í–‰í•˜ì‹ ë‹¤ë‹ˆ ì •ë§ ë©‹ì§€ì„¸ìš”! ğŸ‘ğŸ‘ ê±´ê°•í•œ ì‹ìŠµê´€ê³¼ ìœ ì‚°ì†Œ ìš´ë™ì´ ë‹¤ì´ì–´íŠ¸ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì•Œê³  ê³„ì‹ ê°€ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ê·¸ë˜ ì‹œê°„ì´ ì—†ìœ¼ë‹ˆ, ëŒ€í™”ëŠ” ì´ì¯¤ í•˜ì. ë‚˜ê°€ë³¼í…Œë‹ˆ ë” ì•ˆë¬¼ì–´ë´ë„ ë¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ê±´ê°•í•˜ê³  í–‰ë³µí•œ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "his_log = ''\n",
    "cnt = 0 \n",
    "print('ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”! ')\n",
    "while True:\n",
    "    query = input('[### YOU]: ')\n",
    "    prompt = data['prompt'].format(history=his_log, instruction=query)\n",
    "    result = inference_output(prompt)\n",
    "    response = result.split('\\n\\n'+data['response_split'])[1]\n",
    "    response = response.replace('<|endoftext|>', '').strip()\n",
    "    print()\n",
    "    print('[### Chatbot]: ',response)\n",
    "    print()\n",
    "    if cnt == 0:\n",
    "        his_log = query  +'\\n'+ response\n",
    "    else:\n",
    "        his_log = his_log +'\\n'+query + '\\n' + response\n",
    "    cnt += 1 \n",
    "    #end = int(input('ëë‚¼êº¼ë©´ 0 ì…ë ¥'))\n",
    "    #if end == 0:\n",
    "    if cnt == 10:\n",
    "        print('ëŒ€í™”ì¢…ë£Œ')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529be3c-c6ec-4cc7-b1dc-7949b3f188ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”! \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì•ˆë…•í•˜ì„¸ìš”...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì ì„ ì˜ëª» ìì„œ ê·¸ëŸ°ì§€ ì¡¸ë¦½ê³  í”¼ê³¤í•´ì„œ ì‹œì‘ë¶€í„° í˜ë“œë„¤ìš”.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ì ì„ ì˜ëª» ìì„œ í”¼ê³¤í•˜ë‹¤ë‹ˆ, ì‹œì‘ë¶€í„° í˜ë“œì‹œê² ì–´ìš”. ğŸ˜¢ ì ì„ ì˜ ëª» ìëŠ” ê²ƒì€ ì •ë§ í˜ë“  ì¼ì´ì£ . ì–´ë–¤ ì´ìœ ë¡œ ì ì„ ì˜ëª» ì¤ëŠ”ì§€ ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì´ë”°ê°€ ë°¥ ë¨¹ì„ë•Œ ë§›ìˆëŠ” ìŒì‹ì´ë¼ë„ ë¨¹ê³  ê¸°ìš´ë‚´ì•¼ì£  ë­.  ìš”ì¦˜ ë¬¼ê°€ê°€ ì›Œë‚™ ì˜¤ë¥´ë‹¤ë³´ë‹ˆ ì„ íƒì§€ê°€ ë§ˆë•…ì¹˜ ì•Šë„¤ìš”. ê´€ë ¨ ê¸°ì‚¬ë„ ìˆëŠ”ë° ìš”ì•½í•´ì¤„ìˆ˜ ìˆì–´ìš”?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ë¬¼ê°€ ìƒìŠ¹ìœ¼ë¡œ ì¸í•´ ì‹ë¹„ ë¶€ë‹´ì´ ì»¤ì§„ ìƒí™©, ì •ë§ í˜ë“œì‹œê² ì–´ìš”. ğŸ˜” ì–´ë–¤ ìŒì‹ì„ ì„ íƒí•´ì•¼ í• ì§€ ê³ ë¯¼ì´ ë˜ì‹œë‚˜ìš”? í˜¹ì‹œ ì§‘ì—ì„œ ì§ì ‘ ë§Œë“¤ì–´ ë¨¹ëŠ” ê²ƒì— ëŒ€í•´ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ë°‘ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.  [ì„œìš¸=ë‰´ì‹œìŠ¤]ë‚¨ì£¼í˜„ ê¸°ì = ê°ê·¤ê³¼ ì‚¬ê³¼ ë“± ë†ì‚°ë¬¼ ë¬¼ê°€ê°€ í¬ê²Œ ë›°ê³ , ì‚°ì—…ìš© ë„ì‹œê°€ìŠ¤ ìš”ê¸ˆ ì˜¤ë¦„ì„¸ì— ìƒì‚°ìë¬¼ê°€ê°€ ë‘ ë‹¬ ì—°ì† ìƒìŠ¹í–ˆë‹¤.  ìƒì‚°ìë¬¼ê°€ëŠ” í†µìƒ 1~3ê°œì›” ì‹œì°¨ë¥¼ ë‘ê³  ì†Œë¹„ìë¬¼ê°€ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë§Œí¼ ë¬¼ê°€ ì•ˆì •ì— ë¶€ì •ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.  21ì¼ í•œêµ­ì€í–‰ì´ ë°œí‘œí•œ 1ì›” ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜ëŠ” 121.80(2015ë…„ 100ê¸°ì¤€)ìœ¼ë¡œ ì „ì›” ëŒ€ë¹„ 0.5% ì˜¬ëë‹¤. ì œ1ì°¨ê¸ˆì†ì œí’ˆ, ìŒì‹ë£Œí’ˆ ë“±ì´ ë‚´ë ¸ì§€ë§Œ ë†ì‚°ë¬¼ê³¼ ì„œë¹„ìŠ¤ ë“±ì´ ì˜¤ë¥¸ ì´ìœ ê°€ í¬ë‹¤.  ìƒì‚°ìë¬¼ê°€ì˜ ì „ì›” ëŒ€ë¹„ ìƒìŠ¹ë¥ ì€ ì§€ë‚œí•´ 7ì›”(0.3%)ë¶€í„° 8ì›”(0.9%)ê³¼ 9ì›”(0.5%) ì„ë‹¬ ì—°ì† ë°˜ë“±í–ˆì§€ë§Œ, ìœ ê°€ ë‚´ë¦¼ì„¸ì— 10ì›”(-0.1%)ê³¼ 11ì›”(-0.4%) í•˜ë½í•œ í›„ 12ì›”(0.1%) ë‹¤ì‹œ ë°˜ë“±í•œ ë°” ìˆë‹¤.  ìƒì‚°ìë¬¼ê°€ëŠ” ìƒì‚°ìê°€ ì‹œì¥ì— ê³µê¸‰í•˜ëŠ” ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ ë“±ì˜ ê°€ê²© ë³€ë™ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì˜ ì„ í–‰ì§€í‘œë¡œ í™œìš©ëœë‹¤.  ì „ë…„ ë™ì›”ëŒ€ë¹„ë¡œëŠ” 1.3% ì˜¬ë¼ 6ê°œì›” ì—°ì† ì˜¬ëë‹¤. ìƒì‚°ìë¬¼ê°€ì˜ ì „ë…„ëŒ€ë¹„ ìƒìŠ¹ë¥ ì€ ì§€ë‚œí•´ 6ì›” -0.3%ë¡œ 31ê°œì›” ë§Œì— ë§ˆì´ë„ˆìŠ¤ë¥¼ ê¸°ë¡í•œ í›„ 7ì›”(-0.3%)ì—ë„ í•˜ë½í–ˆì§€ë§Œ, 8ì›”(1.0%)ë¶€í„° ìƒìŠ¹ ì „í™˜í–ˆë‹¤.  ë¶€ë¬¸ë³„ë¡œëŠ” ë†ë¦¼ìˆ˜ì‚°í’ˆ ì§€ìˆ˜ê°€ ì „ì›”ëŒ€ë¹„ 3.8% ì˜¤ë¥¸ 151.26ì„ ê¸°ë¡í•´ ì—­ëŒ€ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆë‹¤. ì¶•ì‚°ë¬¼(-1.3%)ì´ ë‚´ë ¸ì§€ë§Œ, ë†ì‚°ë¬¼(8.3%)ê³¼ ìˆ˜ì‚°ë¬¼(0.2%)ì´ ì˜¤ë¥´ë©´ì„œë‹¤.  ì„¸ë¶€ì ìœ¼ë¡œ ê°ê·¤ì€ ì „ì›”ëŒ€ë¹„ 48.8%, ì‚¬ê³¼ëŠ” 7.5% ì˜¬ëê³ , ëƒ‰ë™ì˜¤ì§•ì–´(2.8%)ê³¼ ê¹€(6.8%)ë„ ìƒìŠ¹í–ˆë‹¤.  ê³µì‚°í’ˆì€ 1ì°¨ê¸ˆì†ì œí’ˆ(-1.0%), ìŒì‹ë£Œí’ˆ(-0.3%)ì´ ë‚´ë ¸ì§€ë§Œ, ì„íƒ„ë°ì„ìœ ì œí’ˆ(0.5%)ê³¼ ì»´í“¨í„°Â·ì „ìë°ê´‘í•™ê¸°ê¸°(0.9%) ë“±ì´ ì˜¬ë¼ ì „ì›”ëŒ€ë¹„ 0.1% ìƒìŠ¹í–ˆë‹¤.  ì „ë ¥Â·ê°€ìŠ¤Â·ìˆ˜ë„ë°íê¸°ë¬¼ì€ ì‚°ì—…ìš©ë„ì‹œê°€ìŠ¤(10.0%) ë“±ì´ ì˜¬ë¼ ì „ì›”ëŒ€ë¹„ 1.0% ìƒìŠ¹í–ˆê³ , ì„œë¹„ìŠ¤ëŠ” ì •ë³´í†µì‹ ë°ë°©ì†¡ì„œë¹„ìŠ¤(1.6%), ì‚¬ì—…ì§€ì›ì„œë¹„ìŠ¤(1.1%), ë¶€ë™ì‚°ì„œë¹„ìŠ¤(0.2%) ë“±ì´ ìƒìŠ¹í•˜ë©° ì „ì›”ëŒ€ë¹„ 0.6% ì˜¬ëë‹¤.  ìœ ì„±ìš± í•œì€ ê²½ì œí†µê³„êµ­ ë¬¼ê°€í†µê³„íŒ€ì¥ì€ \"ê³¼ì‹¤ë¥˜ ìƒìŠ¹ì˜ ì£¼ëœ ì´ìœ ëŠ” ì‘í™© ë¶€ì§„ìœ¼ë¡œ ì œì²  ê³¼ì¼ì— ëŒ€í•œ ìˆ˜ìš”ë„ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤\"ë©´ì„œ \"ê°ê·¤ì€ ëŒ€ì²´ ìˆ˜ìš”ê°€ ë˜ëŠ” ì‚¬ê³¼ì™€ ë°° ë¬¼ê°€ê°€ ì˜¤ë¥´ë©´ì„œ ê°™ì´ ì˜¬ëë‹¤\"ê³  ë§í–ˆë‹¤.  ìƒì‚°ìë¬¼ê°€ì™€ ìˆ˜ì…ë¬¼ê°€ì§€ìˆ˜ë¥¼ ê²°í•©í•´ ì‚°ì¶œí•œ 1ì›” êµ­ë‚´ ê³µê¸‰ë¬¼ê°€ì§€ìˆ˜ëŠ” ì „ì›” ëŒ€ë¹„ 0.5% ìƒìŠ¹í–ˆë‹¤. ì „ë…„ ë™ì›”ê³¼ ë¹„êµí•´ì„œëŠ” 1.4% ë–¨ì–´ì¡Œë‹¤. ì›ì¬ë£Œ(-1.5%)ê°€ ë‚´ë ¸ê³ , ì¤‘ê°„ì¬(0.6%)ì™€ ìµœì¢…ì¬(0.8%)ëŠ” ìƒìŠ¹í–ˆë‹¤.  êµ­ë‚´ ì¶œí•˜ë¥¼ ì œì™¸í•œ ìˆ˜ì¶œì„ í¬í•¨í•˜ëŠ” ì´ì‚°ì¶œ ê¸°ì¤€ìœ¼ë¡œ ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ ê°€ê²© ë³€ë™ì„ ì¸¡ì •í•œ ì´ì‚°ì¶œ ë¬¼ê°€ì§€ìˆ˜ëŠ” ì „ì›” ëŒ€ë¹„ 0.1% ìƒìŠ¹í–ˆë‹¤. ê³µì‚°í’ˆ(1.1%), ì„œë¹„ìŠ¤(0.6%), ë†ë¦¼ìˆ˜ì‚°í’ˆ(3.8%) ë“±ì´ ì˜¬ëë‹¤.   â—ê³µê°ì–¸ë¡  ë‰´ì‹œìŠ¤ njh32@newsis.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ìƒì‚°ìë¬¼ê°€ëŠ” ìƒì‚°ìê°€ ì‹œì¥ì— ê³µê¸‰í•˜ëŠ” ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ ë“±ì˜ ê°€ê²© ë³€ë™ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì˜ ì„ í–‰ì§€í‘œë¡œ í™œìš©ëœë‹¤. ë¶€ë¬¸ë³„ë¡œëŠ” ë†ë¦¼ìˆ˜ì‚°í’ˆ ì§€ìˆ˜ê°€ ì „ì›”ëŒ€ë¹„ 3.8% ì˜¤ë¥¸ 151.26ì„ ê¸°ë¡í•´ ì—­ëŒ€ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆë‹¤.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ìœ„ì˜ ë‚´ìš©ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³ , ê·¸ê²ƒì´ ê¸ì •, ì¤‘ë¦½, ì•„ë‹ˆë©´ ë¶€ì •ì¸ì§€ ê²°ì •í•˜ê³  ëŒ€ë‹µí•´ì£¼ì„¸ìš”. í•´ë‹¹ ê°ì • ë ˆì´ë¸”ì€ \"ê¸ì •\", \"ì¤‘ë¦½\" ë˜ëŠ” \"ë¶€ì •\"ì…ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ê¸ì •\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì¢‹ì•„í•˜ëŠ” ê°ê·¤ë„ ë¨¹ê¸° í˜ë“¤ê² ë„¤ìš”... ê·¸ë˜ë„ ì—´ì‹¬íˆ ì‚´ì•„ì•¼ì£ .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ë§ì•„ìš”, ë¬¼ê°€ê°€ ìƒìŠ¹í•˜ë©´ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì¡°ì°¨ë„ ë¶€ë‹´ìŠ¤ëŸ¬ì›Œì§€ëŠ”êµ°ìš”. ğŸ˜¢ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—´ì‹¬íˆ ì‚´ì•„ì•¼ í•œë‹¤ëŠ” ë§ˆìŒê°€ì§, ì •ë§ ë©‹ì§€ì‹­ë‹ˆë‹¤. ê°ê·¤ ì™¸ì—ë„ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì´ ìˆë‚˜ìš”?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[### YOU]:  ì´ë§Œ, ëŒ€í™”ëŠ” ì¢…ë£Œí•´ì£¼ì„¸ìš”. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[### Chatbot]:  ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ğŸ˜Š ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "his_log = ''\n",
    "cnt = 0 \n",
    "print('ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”! ')\n",
    "while True:\n",
    "    query = input('[### YOU]: ')\n",
    "    prompt = data['prompt'].format(history=his_log, instruction=query)\n",
    "    result = inference_output(prompt)\n",
    "    response = result.split('\\n\\n'+data['response_split'])[1]\n",
    "    response = response.replace('<|endoftext|>', '').strip()\n",
    "    print()\n",
    "    print('[### Chatbot]: ',response)\n",
    "    print()\n",
    "    if cnt == 0:\n",
    "        his_log = query  +'\\n'+ response\n",
    "    else:\n",
    "        his_log = his_log +'\\n'+query + '\\n' + response\n",
    "    cnt += 1 \n",
    "    #end = int(input('ëë‚¼êº¼ë©´ 0 ì…ë ¥'))\n",
    "    #if end == 0:\n",
    "    if cnt == 10:\n",
    "        print('ëŒ€í™”ì¢…ë£Œ')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc1b70-6c5e-44c9-8cd7-9f7591e625bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7331da-6df4-4d60-8121-a0ff3229e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d9b3a-2af7-4658-b8ba-391c9107d8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd6e5f6-e16f-4d5f-826e-5dafb678931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_classification_dataset(c_path):\n",
    "  # Read the CSV file into a pandas DataFrame\n",
    "  df = pd.read_csv(c_path)\n",
    "  # Replace values in the 'labels' column\n",
    "  df['labels'] = df['labels'].replace({'neutral': 'ì¤‘ë¦½', 'negative': 'ë¶€ì •', 'positive': 'ê¸ì •'})\n",
    "\n",
    "  ## í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹í™”\n",
    "\n",
    "  # Hugging Faceì˜ dataset í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
    "  huggingface_dataset = Dataset.from_pandas(df.rename(columns={'labels': 'output', 'kor_sentence': 'input'}))\n",
    "\n",
    "  # instruction ì¶”ê°€ ë° ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½\n",
    "  huggingface_dataset = huggingface_dataset.map(\n",
    "      lambda example: {\"instruction\": 'ë°‘ì˜ ë‚´ìš©ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³ , ê·¸ê²ƒì´ ê¸ì •, ì¤‘ë¦½, ì•„ë‹ˆë©´ ë¶€ì •ì¸ì§€ ê²°ì •í•˜ê³  ëŒ€ë‹µí•´ì£¼ì„¸ìš”. í•´ë‹¹ ê°ì • ë ˆì´ë¸”ì€ \"ê¸ì •\", \"ì¤‘ë¦½\" ë˜ëŠ” \"ë¶€ì •\"ì…ë‹ˆë‹¤.',\n",
    "                       \"input\": example[\"input\"],\n",
    "                       \"output\": example[\"output\"]\n",
    "                       },\n",
    "      remove_columns=['sentence']\n",
    "  )\n",
    "\n",
    "  # í•„ë“œì™€ ê°’ì„ ì¶”ê°€\n",
    "  sentiment_dataset = huggingface_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'github/ukairia777', 'type': 'task_classification'})\n",
    "\n",
    "  return sentiment_dataset\n",
    "\n",
    "def task_summarization_dataset(s_path):\n",
    "  folder_path  = s_path\n",
    "\n",
    "  # Initialize an empty list to store data\n",
    "  data_list = []\n",
    "\n",
    "  # Get a list of all files in the folder\n",
    "  file_list = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "  # Loop through each JSON file and extract 'passage' and 'summary2'\n",
    "  for file_name in file_list:\n",
    "      file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "      # Read the JSON file\n",
    "      with open(file_path, 'r', encoding='utf-8') as file:\n",
    "          data = json.load(file)\n",
    "\n",
    "      # Extract 'passage' and 'summary2' fields\n",
    "      passage = data.get('Meta(Refine)', {}).get('passage', '')\n",
    "      summary2 = data.get('Annotation', {}).get('summary2', '')\n",
    "\n",
    "      #if len(summary2.split('. ')) <= 1: # ì›ì¸ ë¶ˆëª… ?  ê·¸ëƒ¥ ë¬¸ì¥ ê¸¸ì´ ë¯¸ë§Œì¸ ê±°ë§Œ ì§œë¥´ë„ë¡\n",
    "      if len(summary2) <= 100: # ë¬¸ì¥ ê¸¸ì´ê°€ 100 ì´í•˜ì¸ ê²ƒì€ ë²„ë¦¬ê¸°\n",
    "        continue\n",
    "\n",
    "      # Append data to the list\n",
    "      data_list.append({'file_name': file_name, 'body': passage, 'summary': summary2})\n",
    "\n",
    "  # Convert the list of dictionaries to a pandas DataFrame\n",
    "  df = pd.DataFrame(data_list)\n",
    "\n",
    "  # Hugging Faceì˜ datasetìœ¼ë¡œ ë³€í™˜\n",
    "  hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "  # í•„ìš”í•œ í˜•íƒœë¡œ ë³€í™˜\n",
    "  hf_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': 'ë°‘ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.',\n",
    "                       'input': example['body'],\n",
    "                       'output': example['summary']\n",
    "                       },\n",
    "      remove_columns=['file_name','body','summary']\n",
    "                     )\n",
    "  # í•„ë“œì™€ ê°’ì„ ì¶”ê°€\n",
    "  summary_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'aihub', 'type': 'task_summarization'})\n",
    "  return summary_dataset\n",
    "\n",
    "\n",
    "def task_combined_dataset(c_dataset, s_dataset):\n",
    "  # ë‘ ë°ì´í„°ì…‹ í•©ì¹˜ê¸°\n",
    "  combined_dataset = concatenate_datasets([c_dataset, s_dataset])\n",
    "  # ë°ì´í„°ì…‹ ì„ê¸°\n",
    "  combined_dataset = combined_dataset.shuffle(seed=42)  # seed ê°’ì€ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "  # instructionê³¼ inputì„ í•©ì³ì„œ ìƒˆë¡œìš´ instructionì— ê°’ì„ ì£¼ê¸°\n",
    "  combined_dataset = combined_dataset.map(lambda example: {'new_instruction': example['instruction'].replace('#','').strip() + '\\n\\n' + example['input'].rstrip(), 'output': example['output'], 'source': example['source'], 'type': example['type']})\n",
    "\n",
    "  # ê¸°ì¡´ì˜ instructionê³¼ input ì‚­ì œ\n",
    "  combined_dataset = combined_dataset.remove_columns(['instruction', 'input'])\n",
    "  # 'new_instruction' í•„ë“œì˜ ì´ë¦„ì„ 'instruction'ìœ¼ë¡œ ë³€ê²½\n",
    "  combined_dataset = combined_dataset.rename_column('new_instruction', 'instruction')\n",
    "\n",
    "  return combined_dataset\n",
    "\n",
    "\n",
    "class Chat_prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            template_name =  'template_v002' # \"multi\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name, encoding='utf-8') as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "\n",
    "        # ë©€í‹° í„´ ëŒ€í™” ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„\n",
    "        def converter(sentence):\n",
    "            result = re.sub(r\"ì§ˆë¬¸\\s*\", \"### ëª…ë ¹ì–´\", sentence)\n",
    "            result = re.sub(r\"ë‹µë³€\\s*\", \"### ì‘ë‹µ\", result)\n",
    "\n",
    "            return result\n",
    "\n",
    "        instruction = converter(instruction)\n",
    "        new_instruction = instruction.split('\\n')[-1]\n",
    "        history = instruction[:-len(new_instruction)]\n",
    "        try:\n",
    "            new_instruction = new_instruction.split('### ëª…ë ¹ì–´: ')[1]\n",
    "        except:\n",
    "            new_instruction = new_instruction.split('### ëª…ë ¹ì–´: ')[0]\n",
    "\n",
    "        res = self.template[\"prompt\"].format(history=history, instruction=new_instruction)\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "def chat_load_and_preprocess_data(dataset: dataset, tokenizer: tokenizer):\n",
    "    data = dataset\n",
    "    prompter = Chat_prompter(template_name = 'template_v002',\n",
    "                        verbose = False)\n",
    "    # prompter.template =>\n",
    "    def generate_and_tokenize_prompt(instruction, output):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            instruction = instruction,\n",
    "            label = output,\n",
    "        )\n",
    "        #full_prompt_add_eos_token = '<s>\\n' + full_prompt + '\\n</s>'\n",
    "        #full_prompt_add_eos_token = full_prompt + '</s>'\n",
    "        full_prompt_add_eos_token = full_prompt + '<|endoftext|>'\n",
    "        return full_prompt_add_eos_token\n",
    "\n",
    "    original_dataset = data['train']\n",
    "    # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    new_dataset = {\n",
    "      'instruction': original_dataset['instruction'],\n",
    "      'output': original_dataset['output'],\n",
    "      'source': original_dataset['source'],\n",
    "      'type': original_dataset['type'],\n",
    "      'text': [generate_and_tokenize_prompt(i,o) for i, o in zip(original_dataset['instruction'], original_dataset['output'])]\n",
    "    }\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±\n",
    "    final_dataset = Dataset.from_dict(new_dataset)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "class Task_prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            template_name =  'template_v002' # \"multi\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name, encoding='utf-8') as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "\n",
    "        # ë©€í‹° í„´ ëŒ€í™” ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„\n",
    "        def converter(sentence):\n",
    "            result = re.sub(r\"ì§ˆë¬¸\\s*\", \"### ëª…ë ¹ì–´\", sentence)\n",
    "            result = re.sub(r\"ë‹µë³€\\s*\", \"### ì‘ë‹µ\", result)\n",
    "\n",
    "            return result\n",
    "\n",
    "        instruction = converter(instruction)\n",
    "        history = ''\n",
    "\n",
    "        res = self.template[\"prompt\"].format(history=history, instruction=instruction)\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "def task_load_and_preprocess_data(dataset: dataset, tokenizer: tokenizer):\n",
    "    data = dataset\n",
    "    prompter = Task_prompter(template_name = 'template_v002',\n",
    "                        verbose = False)\n",
    "    # prompter.template =>\n",
    "    def generate_and_tokenize_prompt(instruction, output):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            instruction = instruction,\n",
    "            label = output,\n",
    "        )\n",
    "\n",
    "        full_prompt_add_eos_token = full_prompt + '<|endoftext|>'\n",
    "        return full_prompt_add_eos_token\n",
    "\n",
    "    original_dataset = data['train']\n",
    "    # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    new_dataset = {\n",
    "      'instruction': original_dataset['instruction'],\n",
    "      'output': original_dataset['output'],\n",
    "      'source': original_dataset['source'],\n",
    "      'type': original_dataset['type'],\n",
    "      'text': [generate_and_tokenize_prompt(i,o) for i, o in zip(original_dataset['instruction'], original_dataset['output'])]\n",
    "    }\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±\n",
    "    final_dataset = Dataset.from_dict(new_dataset)\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecf860-1d87-4576-bbe9-84d7d56aeffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oosij",
   "language": "python",
   "name": "oosij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
